<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Parte 1 Optimización de Procesos | Inteligencia Artificial aplicada a Negocios y Empresas</title>
  <meta name="description" content="Asienta las bases para convertirte en el Data Scientist del futuro con todo el contenido de estadística descriptiva del curso. En particular verás los mismos contenidos que explicamos en primero de carrera a matemáticos, ingenieros, economistas, biólogos, médicos o informáticos." />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="Parte 1 Optimización de Procesos | Inteligencia Artificial aplicada a Negocios y Empresas" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://www.udemy.com/course/ia4business/?couponCode=B85F8D52148DF5AAD8F7" />
  <meta property="og:image" content="https://www.udemy.com/course/ia4business/?couponCode=B85F8D52148DF5AAD8F7Images/Course_Image.png" />
  <meta property="og:description" content="Asienta las bases para convertirte en el Data Scientist del futuro con todo el contenido de estadística descriptiva del curso. En particular verás los mismos contenidos que explicamos en primero de carrera a matemáticos, ingenieros, economistas, biólogos, médicos o informáticos." />
  <meta name="github-repo" content="https://github.com/joanby/ia4business" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Parte 1 Optimización de Procesos | Inteligencia Artificial aplicada a Negocios y Empresas" />
  
  <meta name="twitter:description" content="Asienta las bases para convertirte en el Data Scientist del futuro con todo el contenido de estadística descriptiva del curso. En particular verás los mismos contenidos que explicamos en primero de carrera a matemáticos, ingenieros, economistas, biólogos, médicos o informáticos." />
  <meta name="twitter:image" content="https://www.udemy.com/course/ia4business/?couponCode=B85F8D52148DF5AAD8F7Images/Course_Image.png" />

<meta name="author" content="Hadelin de Ponteves y Kirill Ermenko" />


<meta name="date" content="2020-04-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="Images/apple-icon-120x120.png" />
  <link rel="shortcut icon" href="Images/favicon.ico" type="image/x-icon" />
<link rel="prev" href="index.html"/>
<link rel="next" href="minimización-de-costes.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Inteligencia Artificial aplicada Negocios y Empresas</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introducción</a></li>
<li class="chapter" data-level="1" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html"><i class="fa fa-check"></i><b>1</b> Optimización de Procesos</a><ul>
<li class="chapter" data-level="1.1" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#caso-práctico-optimización-de-tareas-en-un-almacén-de-comercio-electrónico"><i class="fa fa-check"></i><b>1.1</b> Caso Práctico: Optimización de tareas en un almacén de comercio electrónico</a><ul>
<li class="chapter" data-level="1.1.1" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#problema-a-resolver"><i class="fa fa-check"></i><b>1.1.1</b> Problema a resolver</a></li>
<li class="chapter" data-level="1.1.2" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#entorno-a-definir"><i class="fa fa-check"></i><b>1.1.2</b> Entorno a definir</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#solución-de-inteligencia-artificial"><i class="fa fa-check"></i><b>1.2</b> Solución de Inteligencia Artificial</a><ul>
<li class="chapter" data-level="1.2.1" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#proceso-de-decisión-de-markov"><i class="fa fa-check"></i><b>1.2.1</b> Proceso de Decisión de Markov</a></li>
<li class="chapter" data-level="1.2.2" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#q-learning"><i class="fa fa-check"></i><b>1.2.2</b> Q-Learning</a></li>
<li class="chapter" data-level="1.2.3" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#el-algoritmo-de-q-learning-al-completo"><i class="fa fa-check"></i><b>1.2.3</b> El algoritmo de Q-Learning al completo</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#implementación"><i class="fa fa-check"></i><b>1.3</b> Implementación</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="minimización-de-costes.html"><a href="minimización-de-costes.html"><i class="fa fa-check"></i><b>2</b> Minimización de Costes</a><ul>
<li class="chapter" data-level="2.1" data-path="minimización-de-costes.html"><a href="minimización-de-costes.html#caso-práctico-minimización-de-costes-en-el-consumo-energético-de-un-centro-de-datos"><i class="fa fa-check"></i><b>2.1</b> Caso Práctico: Minimización de Costes en el Consumo Energético de un Centro de Datos</a><ul>
<li class="chapter" data-level="2.1.1" data-path="minimización-de-costes.html"><a href="minimización-de-costes.html#problema-a-resolver-1"><i class="fa fa-check"></i><b>2.1.1</b> Problema a resolver</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="minimización-de-costes.html"><a href="minimización-de-costes.html#solución-de-ia"><i class="fa fa-check"></i><b>2.2</b> Solución de IA</a><ul>
<li class="chapter" data-level="2.2.1" data-path="minimización-de-costes.html"><a href="minimización-de-costes.html#q-learning-en-deep-learning"><i class="fa fa-check"></i><b>2.2.1</b> Q-Learning en Deep Learning</a></li>
<li class="chapter" data-level="2.2.2" data-path="minimización-de-costes.html"><a href="minimización-de-costes.html#experience-replay"><i class="fa fa-check"></i><b>2.2.2</b> Experience Replay</a></li>
<li class="chapter" data-level="2.2.3" data-path="minimización-de-costes.html"><a href="minimización-de-costes.html#el-cerebro"><i class="fa fa-check"></i><b>2.2.3</b> El cerebro</a></li>
<li class="chapter" data-level="2.2.4" data-path="minimización-de-costes.html"><a href="minimización-de-costes.html#el-algoritmo-de-deep-q-learning-al-completo"><i class="fa fa-check"></i><b>2.2.4</b> El algoritmo de Deep Q-Learning al completo</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="minimización-de-costes.html"><a href="minimización-de-costes.html#implementation"><i class="fa fa-check"></i><b>2.3</b> Implementation</a><ul>
<li class="chapter" data-level="2.3.1" data-path="minimización-de-costes.html"><a href="minimización-de-costes.html#paso-1-construcción-del-entorno"><i class="fa fa-check"></i><b>2.3.1</b> Paso 1: Construcción del Entorno</a></li>
<li class="chapter" data-level="2.3.2" data-path="minimización-de-costes.html"><a href="minimización-de-costes.html#paso-2-construcción-del-cerebro"><i class="fa fa-check"></i><b>2.3.2</b> Paso 2: Construcción del cerebro</a></li>
<li class="chapter" data-level="2.3.3" data-path="minimización-de-costes.html"><a href="minimización-de-costes.html#paso-3-implementación-del-algoritmo-de-deep-reinforcement-learning"><i class="fa fa-check"></i><b>2.3.3</b> Paso 3: Implementación del algoritmo de Deep Reinforcement Learning</a></li>
<li class="chapter" data-level="2.3.4" data-path="minimización-de-costes.html"><a href="minimización-de-costes.html#paso-4-entrenar-la-ia"><i class="fa fa-check"></i><b>2.3.4</b> Paso 4: Entrenar la IA</a></li>
<li class="chapter" data-level="2.3.5" data-path="minimización-de-costes.html"><a href="minimización-de-costes.html#paso-5-probar-nuestra-ia"><i class="fa fa-check"></i><b>2.3.5</b> Paso 5: Probar nuestra IA</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="minimización-de-costes.html"><a href="minimización-de-costes.html#resumen-el-algoritmo-general-de-ia"><i class="fa fa-check"></i><b>2.4</b> Resumen: El Algoritmo General de IA</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="maximización-de-beneficios-revenues.html"><a href="maximización-de-beneficios-revenues.html"><i class="fa fa-check"></i><b>3</b> Maximización de Beneficios Revenues</a><ul>
<li class="chapter" data-level="3.1" data-path="maximización-de-beneficios-revenues.html"><a href="maximización-de-beneficios-revenues.html#caso-práctico-maximización-de-beeficios-de-un-negocio-de-venta-online-en-línea"><i class="fa fa-check"></i><b>3.1</b> Caso Práctico: Maximización de beeficios de un negocio de venta online en línea</a><ul>
<li class="chapter" data-level="3.1.1" data-path="maximización-de-beneficios-revenues.html"><a href="maximización-de-beneficios-revenues.html#problema-a-reesolver"><i class="fa fa-check"></i><b>3.1.1</b> Problema a reesolver</a></li>
<li class="chapter" data-level="3.1.2" data-path="maximización-de-beneficios-revenues.html"><a href="maximización-de-beneficios-revenues.html#definición-del-entorno"><i class="fa fa-check"></i><b>3.1.2</b> Definición del Entorno</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="maximización-de-beneficios-revenues.html"><a href="maximización-de-beneficios-revenues.html#solución-de-ia-1"><i class="fa fa-check"></i><b>3.2</b> Solución de IA</a></li>
<li class="chapter" data-level="3.3" data-path="maximización-de-beneficios-revenues.html"><a href="maximización-de-beneficios-revenues.html#implementación-1"><i class="fa fa-check"></i><b>3.3</b> Implementación</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="conclusión.html"><a href="conclusión.html"><i class="fa fa-check"></i>Conclusión</a></li>
<li class="chapter" data-level="4" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html"><i class="fa fa-check"></i><b>4</b> Anexos adicionales</a><ul>
<li class="chapter" data-level="4.1" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#anexo-1-redes-neuronales-artificiales"><i class="fa fa-check"></i><b>4.1</b> Anexo 1: Redes Neuronales Artificiales</a><ul>
<li class="chapter" data-level="4.1.1" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#la-neurona"><i class="fa fa-check"></i><b>4.1.1</b> La Neurona</a></li>
<li class="chapter" data-level="4.1.2" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#la-función-de-activación"><i class="fa fa-check"></i><b>4.1.2</b> La Función de Activación</a></li>
<li class="chapter" data-level="4.1.3" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#cómo-funcionan-las-redes-neuronales"><i class="fa fa-check"></i><b>4.1.3</b> ¿Cómo funcionan las Redes Neuronales?</a></li>
<li class="chapter" data-level="4.1.4" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#cómo-aprenden-las-redes-neuronales"><i class="fa fa-check"></i><b>4.1.4</b> ¿Cómo aprenden las Redes Neuronales?</a></li>
<li class="chapter" data-level="4.1.5" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#propagación-hacia-adelante-and-propagación-hacia-atrás"><i class="fa fa-check"></i><b>4.1.5</b> Propagación hacia adelante and propagación hacia atrás</a></li>
<li class="chapter" data-level="4.1.6" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#gradiente-descendente"><i class="fa fa-check"></i><b>4.1.6</b> Gradiente Descendente</a></li>
<li class="chapter" data-level="4.1.7" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#optimizadores"><i class="fa fa-check"></i><b>4.1.7</b> Optimizadores</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#anexo-2-tres-modelos-adicionales-de-ia"><i class="fa fa-check"></i><b>4.2</b> Anexo 2: Tres modelos adicionales de IA</a><ul>
<li class="chapter" data-level="4.2.1" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#aprendizaje-convolucional-q-profundo"><i class="fa fa-check"></i><b>4.2.1</b> Aprendizaje convolucional Q-profundo</a></li>
<li class="chapter" data-level="4.2.2" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#asynchronous-actor-critic-agents-a3c"><i class="fa fa-check"></i><b>4.2.2</b> Asynchronous Actor-Critic Agents (A3C)</a></li>
<li class="chapter" data-level="4.2.3" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#búsqueda-aleatoria-aumentada"><i class="fa fa-check"></i><b>4.2.3</b> Búsqueda aleatoria aumentada</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#anexo-3-preguntas-y-respuestas"><i class="fa fa-check"></i><b>4.3</b> Anexo 3: Preguntas y Respuestas</a><ul>
<li class="chapter" data-level="4.3.1" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#pr-de-la-parte-1---optimización-de-procesos"><i class="fa fa-check"></i><b>4.3.1</b> P&amp;R de la Parte 1 - Optimización de Procesos</a></li>
<li class="chapter" data-level="4.3.2" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#pr-de-la-parte-2---minimización-de-costes"><i class="fa fa-check"></i><b>4.3.2</b> P&amp;R de la Parte 2 - Minimización de Costes</a></li>
<li class="chapter" data-level="4.3.3" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#pr-de-la-parte-3---maximización-de-beneficios"><i class="fa fa-check"></i><b>4.3.3</b> P&amp;R de la Parte 3 - Maximización de Beneficios</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://www.udemy.com/course/ia4business/?couponCode=B85F8D52148DF5AAD8F7" target="blank">Curso en Udemy</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Inteligencia Artificial aplicada a Negocios y Empresas</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="optimización-de-procesos" class="section level1">
<h1><span class="header-section-number">Parte 1</span> Optimización de Procesos</h1>
<p>Aquí vamos con nuestro primer caso práctico y nuestro primer modelo de IA. ¡Esperamos que estés listo!</p>
<div id="caso-práctico-optimización-de-tareas-en-un-almacén-de-comercio-electrónico" class="section level2">
<h2><span class="header-section-number">1.1</span> Caso Práctico: Optimización de tareas en un almacén de comercio electrónico</h2>
<div id="problema-a-resolver" class="section level3">
<h3><span class="header-section-number">1.1.1</span> Problema a resolver</h3>
<p>El problema a resolver será optimizar los flujos dentro del siguiente almacén:</p>
<p><img src="Images/Warehouse_1.png" style="width:100.0%" /></p>
<p>El almacén pertenece a una empresa online minorista que vende productos a una variedad de clientes. Dentro de este almacén, los productos se almacenan en 12 ubicaciones diferentes, etiquetadas con las siguientes letras de la A a la L:</p>
<p><img src="Images/Warehouse_2.png" style="width:100.0%" /></p>
<p>A medida que los clientes hacen los pedidos online, un robot de almacén autónomo se mueve por el almacén para recoger los productos para futuras entregas. Así es como se ve:</p>
<div class="figure">
<img src="Images/Autonomous_Warehouse_Robot.jpg" alt="Robot de Almacen Autónomo" style="width:100.0%" />
<p class="caption">Robot de Almacen Autónomo</p>
</div>
<p>Las 12 ubicaciones están conectadas a un sistema informático, que clasifica en tiempo real las prioridades de recolección de productos para estas 12 ubicaciones. Por ejemplo, en un momento específico <span class="math inline">\(t\)</span>, devolverá la siguiente clasificación:</p>
<table>
<thead>
<tr class="header">
<th align="center"><strong>Rango de Prioridad</strong></th>
<th align="center"><strong>Ubicación</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">G</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">K</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">L</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">J</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="center">A</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td align="center">I</td>
</tr>
<tr class="odd">
<td align="center">7</td>
<td align="center">H</td>
</tr>
<tr class="even">
<td align="center">8</td>
<td align="center">C</td>
</tr>
<tr class="odd">
<td align="center">9</td>
<td align="center">B</td>
</tr>
<tr class="even">
<td align="center">10</td>
<td align="center">D</td>
</tr>
<tr class="odd">
<td align="center">11</td>
<td align="center">F</td>
</tr>
<tr class="even">
<td align="center">12</td>
<td align="center">E</td>
</tr>
</tbody>
</table>
<p>La ubicación G tiene prioridad 1, lo que significa que es la máxima prioridad, ya que contiene un producto que debe recogerse y entregarse de inmediato. Nuestro robot de almacén autónomo debe moverse a la ubicación G por la ruta más corta, dependiendo de dónde se encuentre. Nuestro objetivo es construir una IA que regrese esa ruta más corta, donde sea que esté el robot. Pero luego, como vemos, las ubicaciones K y L están en las 3 prioridades principales. Por lo tanto, querremos implementar una opción para que nuestro Robot de almacén autónomo pase por algunas ubicaciones intermedias antes de llegar a su ubicación final de máxima prioridad.</p>
<p>La forma en que el sistema calcula las prioridades de las ubicaciones está fuera del alcance de este caso práctico. La razón de esto es que puede haber muchas formas, desde reglas o algoritmos simples, hasta cálculos deterministas y aprendizaje automático. Pero la mayoría de estas formas no serían inteligencia artificial como la conocemos hoy. En lo que realmente queremos centrarnos es en la IA central, que abarca Q-Learning, Deep Q-Learning y otras ramas de Reinforcement Learning. Entonces, solo diremos, por ejemplo, que la ubicación G es la máxima prioridad porque uno de los clientes de platino más leales de la compañía hizo un pedido urgente de un producto almacenado en la ubicación G, que por lo tanto debe entregarse lo antes posible.</p>
<p>Por lo tanto, en conclusión, nuestra misión es construir una IA que siempre tome la ruta más corta a la ubicación de máxima prioridad, sea cual sea la ubicación desde la que comienza, y tener la opción de ir a una ubicación intermedia que se encuentre entre las 3 prioridades principales.</p>
</div>
<div id="entorno-a-definir" class="section level3">
<h3><span class="header-section-number">1.1.2</span> Entorno a definir</h3>
<p>Al construir una IA, lo primero que siempre tenemos que hacer es definir el entorno. Y definir un entorno siempre requiere los tres elementos siguientes:</p>
<ul>
<li>Definir los estados</li>
<li>Definir las acciones</li>
<li>Definir las recompensas</li>
</ul>
<p>Definamos estos tres elementos, uno por uno.</p>
<p><strong>Definir los estados.</strong></p>
<p>Comencemos con los estados. El estado de entrada es simplemente la ubicación donde está nuestro Robot de almacén autónomo en cada momento <span class="math inline">\(t\)</span>. Sin embargo, dado que construiremos nuestra IA con ecuaciones matemáticas, codificaremos los nombres de las ubicaciones (A, B, C, …) en números de índice, con respecto a la siguiente asignación:</p>
<table>
<thead>
<tr class="header">
<th align="center"><strong>Ubicación</strong></th>
<th align="center"><strong>Estado</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">A</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="center">B</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">C</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center">D</td>
<td align="center">3</td>
</tr>
<tr class="odd">
<td align="center">E</td>
<td align="center">4</td>
</tr>
<tr class="even">
<td align="center">F</td>
<td align="center">5</td>
</tr>
<tr class="odd">
<td align="center">G</td>
<td align="center">6</td>
</tr>
<tr class="even">
<td align="center">H</td>
<td align="center">7</td>
</tr>
<tr class="odd">
<td align="center">I</td>
<td align="center">8</td>
</tr>
<tr class="even">
<td align="center">J</td>
<td align="center">9</td>
</tr>
<tr class="odd">
<td align="center">K</td>
<td align="center">10</td>
</tr>
<tr class="even">
<td align="center">L</td>
<td align="center">11</td>
</tr>
</tbody>
</table>
<p>Hay una razón específica por la que codificamos los estados con índices del 0 al 11, en lugar de otros enteros. La razón es que trabajaremos con matrices, una matriz de recompensas y una matriz de valores Q, y cada línea y columna de estas matrices corresponderá a una ubicación específica. Por ejemplo, la primera línea de cada matriz, que tiene el índice 0, corresponde a la ubicación A. La segunda línea / columna, que tiene el índice 1, corresponde a la ubicación B. Etc. Veremos el propósito de trabajar con matrices con más detalles. un poco más tarde.</p>
<p><strong>Definir las acciones.</strong></p>
<p>Ahora definamos las posibles acciones a realizar Las acciones son simplemente los siguientes movimientos que el robot puede hacer para ir de un lugar a otro. Entonces, por ejemplo, digamos que el robot está en la ubicación J, las posibles acciones que el robot puede llevar a cabo es ir a I, F o K. Y nuevamente, ya que trabajaremos con ecuaciones matemáticas, codificaremos estas acciones con los mismos índices que para los estados. Por lo tanto, siguiendo nuestro mismo ejemplo donde el robot está en la ubicación J en un momento específico, las posibles acciones que el robot puede jugar son, de acuerdo con nuestro mapeo anterior anterior: 5, 8 y 10. De hecho, el índice 5 corresponde a F, el índice 8 corresponde a I y el índice 10 corresponde a K. Por lo tanto, eventualmente, la lista total de acciones que la IA puede llevar a cabo en general es la siguiente:</p>
<p><span class="math display">\[actions = [0,1,2,3,4,5,6,7,8,9,10,11]\]</span></p>
<p>Obviamente, al estar en una ubicación específica, hay algunas acciones que el robot no puede llevar a cabo. Tomando el mismo ejemplo anterior, si el robot está en la ubicación J, puede ejecutar las acciones 5, 8 y 10, pero no puede ejecutar las otras acciones. Nos aseguraremos de especificar eso al atribuir una recompensa 0 a las acciones que no puede llevar a cabo, y una recompensa 1 a las acciones que si puede realizar. Y eso nos lleva a las recompensas.</p>
<p><strong>Definir las recompensas.</strong></p>
<p>Lo último que tenemos que hacer ahora para construir nuestro entorno es definir un sistema de recompensas. Más específicamente, tenemos que definir una función de recompensa <span class="math inline">\(R\)</span> que toma como entradas un estado <span class="math inline">\(s\)</span> y una acción <span class="math inline">\(a\)</span>, y devuelve una recompensa numérica que la IA obtendrá al llevar a cabo la acción <span class="math inline">\(a\)</span> en el estado <span class="math inline">\(s\)</span>:</p>
<p><span class="math display">\[R : (\textrm{state}, \textrm{action}) \mapsto r \in \mathbb{R}\]</span></p>
<p>Entonces, ¿cómo vamos a construir esa función para nuestro caso práctico? Aquí esto es simple. Dado que hay un número discreto y finito de estados (los índices de 0 a 11), así como un número discreto y finito de acciones (mismos índices de 0 a 11), la mejor manera de construir nuestra función de recompensa R es simplemente hacer una matriz. Nuestra función de recompensa será exactamente una matriz de 12 filas y 12 columnas, donde las filas corresponden a los estados y las columnas corresponden a las acciones. De esa forma, en nuestra función $R: (s, a) r  in  $, <span class="math inline">\(s\)</span> será el índice de la fila de la matriz, <span class="math inline">\(a\)</span> será el índice de la columna de matriz, y <span class="math inline">\(r\)</span> será la celda de los índices <span class="math inline">\((s, a)\)</span> en la matriz.</p>
<p>Por lo tanto, lo único que tenemos que hacer ahora para definir nuestra función de recompensa es simplemente llenar esta matriz con las recompensas numéricas. Y como acabamos de decir en el párrafo anterior, lo que tenemos que hacer primero es atribuir, para cada una de las 12 ubicaciones, una recompensa 0 por las acciones que el robot no puede ejecutar, y una recompensa 1 por las acciones que el robot puede llevar a cabo. Al hacer eso para cada una de las 12 ubicaciones, terminaremos con una matriz de recompensas. Vamos a construirlo paso a paso, comenzando con la primera ubicación:</p>
<p><em>Ubicación A.</em></p>
<p>Cuando se encuentra en la ubicación A, el robot solo puede ir a la ubicación B. Por lo tanto, dado que la ubicación A tiene el índice 0 (primera fila de la matriz) y la ubicación B tiene el índice 1 (segunda columna de la matriz), la primera fila de la matriz de las recompensas obtendrá un 1 en la segunda columna y un 0 en todas las otras columnas, así:</p>
<p><img src="Images/Rewards_Matrix_1.png" /></p>
<p><em>Ubicación B.</em></p>
<p>Al estar en la ubicación B, el robot solo puede ir a tres ubicaciones diferentes: A, C y F. Dado que B tiene el índice 1 (segunda fila), y A, C, F tienen los índices respectivos 0, 2, 5 (1ra, 3ra. , y sexta columna), entonces la segunda fila de la matriz de recompensas obtendrá un 1 en las columnas 1a, 3a y 6a, y 0 en todas las otras columnas. Por lo tanto obtenemos:</p>
<p><img src="Images/Rewards_Matrix_2.png" /></p>
<p><em>Ubicación C.</em></p>
<p>Ocurre lo mismo, la ubicación C (de índice 2) solo está conectada a B y G (de índices 1 y 6), por lo que la tercera fila de la matriz de recompensas es:</p>
<p><img src="Images/Rewards_Matrix_3.png" /></p>
<p><em>En el resto de ubicaciones…</em></p>
<p>Al hacer lo mismo para todas las demás ubicaciones, finalmente obtenemos nuestra matriz final de recompensas:</p>
<p><img src="Images/Rewards_Matrix_4.png" /></p>
<p>Felicidades, acabamos de definir las recompensas. Lo hicimos simplemente construyendo esta matriz de recompensas. Es importante entender que esta es la forma en que definimos el sistema de recompensas cuando hacemos Q-Learning con un número finito de entradas y acciones. En el Caso Práctico 2, veremos que procederemos de manera muy diferente.</p>
<p>Ya casi hemos terminado, lo único que tenemos que hacer es atribuir grandes recompensas a las ubicaciones de mayor prioridad. Esto lo hará el sistema informático que devuelve las prioridades de recolección de productos para cada una de las 12 ubicaciones. Por lo tanto, dado que la ubicación G es la máxima prioridad, el sistema informático actualizará la matriz de recompensas atribuyendo una alta recompensa en la celda <span class="math inline">\((G, G)\)</span>:</p>
<p><img src="Images/Rewards_Matrix_5.png" /></p>
<p>Y así es como el sistema de recompensas funcionará con Q-Learning. Atribuimos la recompensa más alta (aquí 1000) a la ubicación de máxima prioridad G. Luego puedes ver en las clases de vídeo del curso cómo podemos atribuir una recompensa más alta a la segunda ubicación de mayor prioridad (ubicación K), para hacer que nuestro robot pase por esto ubicación intermedia de máxima prioridad, optimizando así los flujos de movimiento por el almacén.</p>
</div>
</div>
<div id="solución-de-inteligencia-artificial" class="section level2">
<h2><span class="header-section-number">1.2</span> Solución de Inteligencia Artificial</h2>
<p>The AI Solution that will solve the problem described above is a Q-Learning model. Since the latter is based on Markov Decision Processes, or MDPs, we will start by explaining what they are, and then we will move on to the intuition and maths details behind the Q-Learning model.</p>
<div id="proceso-de-decisión-de-markov" class="section level3">
<h3><span class="header-section-number">1.2.1</span> Proceso de Decisión de Markov</h3>
<p>Un Proceso de Decisión de Markov es una tupla <span class="math inline">\((S, A, T, R)\)</span> donde:</p>
<ul>
<li><span class="math inline">\(S\)</span> es el conjunto de los diferentes estados. Por lo tanto, en nuestro caso de estudio: <span class="math display">\[S = \{0,1,2,3,4,5,6,7,8,9,10,11\}\]</span></li>
<li><span class="math inline">\(A\)</span> es el conjunto de las diferentes acciones que se pueden llevar a cabo en cada momento <span class="math inline">\(t\)</span>. Por lo tanto, en nuestro caso de estudio: <span class="math display">\[A = \{0,1,2,3,4,5,6,7,8,9,10,11\}\]</span></li>
<li><span class="math inline">\(T\)</span> es la llamada regla de transición:</li>
</ul>
<p><span class="math display">\[T : (s_t \in S, s_{t+1} \in S, a_t \in A) \mapsto \mathbb{P}(s_{t+1}|s_t,a_t)\]</span></p>
<p>donde <span class="math inline">\(\mathbb {P} (s_{t + 1} | s_t, a_t)\)</span> es la probabilidad de alcanzar el estado futuro <span class="math inline">\(s_{t + 1}\)</span> cuando se lleva a cabo la acción <span class="math inline">\(a_t\)</span> en el estado <span class="math inline">\(s_t\)</span>. Por lo tanto, <span class="math inline">\(T\)</span> es la distribución de probabilidad de los estados futuros en el tiempo <span class="math inline">\(t + 1\)</span> dado el estado actual y la acción ejecutada en el tiempo <span class="math inline">\(t\)</span>. En consecuencia, podemos predecir el estado futuro <span class="math inline">\(s_{t + 1}\)</span> tomando un valor aleatorio de esa distribución <span class="math inline">\(T\)</span>:</p>
<p><span class="math display">\[s_{t+1} \sim T(s_t,.,a_t)\]</span></p>
<p>En nuestro estudio de caso, verás a través de nuestra implementación que esta distribución <span class="math inline">\(T\)</span> de nuestra IA simplemente será la distribución uniforme, que es una opción clásica de distribución que funciona muy bien en el marco del Q-Learning.</p>
<ul>
<li><span class="math inline">\(R\)</span> es la función de recompensas:</li>
</ul>
<p><span class="math display">\[R : (s_t \in S, a_t \in A) \mapsto r_t \in \mathbb{R}\]</span></p>
<p>donde <span class="math inline">\(r_t\)</span> es la recompensa obtenida después de ejecutar la acción <span class="math inline">\(a_t\)</span> en el estado <span class="math inline">\(s_t\)</span>. En nuestro caso práctico, esta función de recompensa es exactamente la matriz que definimos previamente.</p>
<p>Después de definir el proceso de decisión de Markov, ahora es importante recordar que se basa en el siguiente supuesto: la probabilidad del estado futuro <span class="math inline">\(s_{t + 1}\)</span> solamente depende del estado actual <span class="math inline">\(s_t\)</span> y la acción ejecutada <span class="math inline">\(a_t\)</span>, y bajo ningun concepto no depende de ninguno de los estados y acciones anteriores. Es decir:</p>
<p><span class="math display">\[\mathbb{P}(s_{t+1}|s_0,a_0,s_1,a_1,...,s_t,a_t) = \mathbb{P}(s_{t+1}|s_t,a_t)\]</span></p>
<p>En otras palabras, un proceso de decisión de Markov no tiene memoria.</p>
<div style="page-break-after: always;"></div>
<p>Ahora repasemos lo que va a ocurrir en términos de un proceso de decisión de Markov. En cada instante <span class="math inline">\(t\)</span>:</p>
<ul>
<li>La IA observa el estado actual <span class="math inline">\(s_t\)</span>.</li>
<li>La IA ejecuta la acción <span class="math inline">\(a_t\)</span>.</li>
<li>La IA recibe la recompensa <span class="math inline">\(r_t = R(s_t, a_t)\)</span>.</li>
<li>La IA entra en el siguiente estado <span class="math inline">\(s_{t+1}\)</span>.</li>
</ul>
<p>Así que la pregunta clave es:</p>
<blockquote>
<p><strong>¿Cómo sabe la IA qué acción llevar a cabo en cada instante <span class="math inline">\(t\)</span>?</strong></p>
</blockquote>
<p>Para responder a esta pregunta, necesitamos introducir la función de política. La función de política <span class="math inline">\(\pi\)</span> es exactamente la función que, dado un estado <span class="math inline">\(s_t\)</span>, devuelve la acción <span class="math inline">\(a_t\)</span>:</p>
<p><span class="math display">\[\pi: s_t \in S \mapsto a_t \in A\]</span></p>
<p>Denotemos por <span class="math inline">\(\Pi\)</span> el conjunto de todas las funciones de política posibles. Entonces, la elección de las mejores acciones para jugar se convierte en un problema de optimización. De hecho, se trata de encontrar la política óptima <span class="math inline">\(\pi^*\)</span> que maximice la recompensa acumulada:</p>
<p><span class="math display">\[\pi^* = \underset{\pi \in \Pi}{\textrm{argmax}} \sum_{t \ge 0} R(s_t,\pi(s_t))\]</span></p>
<p>Por lo tanto, la pregunta anterior se convierte en:</p>
<blockquote>
<p><strong>¿Cómo encontrar esta política óptima <span class="math inline">\(\pi^*\)</span>?</strong></p>
</blockquote>
<p>Aquí es donde entra en juego el Q-Learning.</p>
</div>
<div id="q-learning" class="section level3">
<h3><span class="header-section-number">1.2.2</span> Q-Learning</h3>
<p>Antes de comenzar a entrar en los detalles de Q-Learning, necesitamos explicar el concepto del valor Q.</p>
<p><strong>El valor Q</strong></p>
<p>Para cada par de estado y acción <span class="math inline">\((s, a)\)</span>, vamos a asociar un valor numérico llamado <span class="math inline">\(Q (s, a)\)</span>:</p>
<p><span class="math display">\[Q: (s \in S, a \in A) \mapsto Q(s,a) \in \mathbb{R}\]</span></p>
<p>Diremos que <span class="math inline">\(Q (s, a)\)</span> es <em>el valor Q de la acción <span class="math inline">\(a\)</span> llevada a cabo en el estado <span class="math inline">\(s\)</span></em>.</p>
<p>Para comprender el propósito de este <em>Valor Q</em>, necesitamos introducir la Diferencia Temporal.</p>
<p><strong>La Diferencia Temporal</strong></p>
<p>Al principio <span class="math inline">\(t = 0\)</span>, todos los valores Q se inicializan a 0:</p>
<p><span class="math display">\[\forall s \in S, a \in A, Q(s,a) = 0\]</span></p>
<p>Ahora supongamos que estamos en el instante <span class="math inline">\(t\)</span>, en cierto estado <span class="math inline">\(s_t\)</span>. Llevamos a cabo una acción aleatoria <span class="math inline">\(a_t\)</span>, que nos lleva al estado <span class="math inline">\(s_{t + 1}\)</span> y obtenemos la recompensa <span class="math inline">\(R(s_t, a_t)\)</span>.</p>
<p>Ahora podemos presentar la diferencia temporal, que básicamente es el corazón de Q-Learning. La diferencia temporal en el tiempo <span class="math inline">\(t\)</span>, denotada por <span class="math inline">\(TD_t(s_t, a_t)\)</span>, es la diferencia entre:</p>
<ul>
<li><p><span class="math inline">\(R(s_t,a_t) + \gamma \underset{a}{\max}(Q(s_{t+1},a))\)</span>, es decir la recompensa <span class="math inline">\(R (s_t, a_t)\)</span> obtenida al llevar a cabo la acción <span class="math inline">\(a_t\)</span> en el estado <span class="math inline">\(s_t\)</span>, más el valor Q de la mejor acción jugada en el estado futuro <span class="math inline">\(s_{t + 1}\)</span>, descontado por un factor $$, llamado factor de descuento.</p></li>
<li><p>y <span class="math inline">\(Q(s_t, a_t)\)</span>, es decir el valor Q de la acción <span class="math inline">\(a_t\)</span> llevada a cabo en el estado $ s_t $,</p></li>
</ul>
<p>que nos lleva a:</p>
<p><span class="math display">\[TD_t(s_t,a_t) = R(s_t,a_t) + \gamma \underset{a}{\max}(Q(s_{t+1},a)) - Q(s_t,a_t)\]</span></p>
<blockquote>
<p><strong>Bien, genial, pero ¿cuál es exactamente el propósito de esta diferencia temporal <span class="math inline">\(TD_t(s_t,a_t)\)</span>?</strong></p>
</blockquote>
<p>Respondamos esta pregunta para darnos una mejor idea de la IA. <span class="math inline">\(TD_t (s_t, a_t)\)</span> es como una recompensa intrínseca. La IA aprenderá los valores Q de tal manera que:</p>
<ul>
<li>Si <span class="math inline">\(TD_t(s_t,a_t)\)</span> es alta, la IA recibe una <em>buena sorpresa</em>.</li>
<li>Si <span class="math inline">\(TD_t(s_t,a_t)\)</span> es alta, la IA recibe <em>frustración</em>.</li>
</ul>
<p>En ese sentido, la IA repetirá algunas actualizaciones de los valores Q (a través de una ecuación llamada la ecuación de Bellman) hacia diferencias temporales más altas.</p>
<p>En consecuencia, en el siguiente paso final del algoritmo Q-Learning, usamos la diferencia temporal para reforzar los pares (estado, acción) desde el tiempo <span class="math inline">\(t-1\)</span> hasta el tiempo <span class="math inline">\(t\)</span>, de acuerdo con la siguiente ecuación:</p>
<p><span class="math display">\[Q_t(s_t,a_t) = Q_{t-1}(s_t,a_t) + \alpha TD_t(s_t,a_t)\]</span></p>
<p>donde <span class="math inline">\(\alpha \in \mathbb{R}\)</span> es la tasa de aprendizaje, que determina qué tan rápido va el aprendizaje de los valores Q o qué tan grandes son las actualizaciones de los mismos. Su valor suele ser un número real elegido entre 0 y 1, como por ejemplo 0.01, 0.05, 0.1 o 0.5. Cuanto menor sea su valor, más pequeñas serán las actualizaciones de los valores Q y más larga será la ejecución del algoritmo de Q-Learning. Cuanto mayor sea su valor, mayores serán las actualizaciones de los valores Q y más rápido será el algoritmo de Q-Learning.</p>
<p>Esta ecuación anterior es la ecuación de Bellman. Es el pilar fundamental del Q-Learning.</p>
<p>Con este punto de vista, los valores Q miden la acumulación de sorpresa o frustración asociada con el par de acciones y estados <span class="math inline">\((s_t, a_t)\)</span>. En el caso de recibir sorpresa, la IA se refuerza, y en el caso de recibir frustración, la IA se debilita. Por lo tanto, queremos aprender los valores Q que le darán a la IA la máxima <em>buena sorpresa</em>.</p>
<p>En consecuencia, la decisión de qué acción ejecutar depende principalmente del valor Q <span class="math inline">\(Q(s_t, a_t)\)</span>. Si la acción <span class="math inline">\(a_t\)</span> ejecutada en el estado <span class="math inline">\(s_t\)</span> está asociada con un valor Q alto <span class="math inline">\(Q (s_t, a_t)\)</span>, la IA tendrá una mayor tendencia a elegir la acción <span class="math inline">\(a_t\)</span>. Por otro lado, si la acción <span class="math inline">\(a_t\)</span> que se ha llevado a cabo en el estado <span class="math inline">\(s_t\)</span> está asociada con un valor Q pequeño <span class="math inline">\(Q(s_t, a_t)\)</span>, la IA tendrá una tendencia menor a elegir la acción <span class="math inline">\(a_t\)</span>.</p>
<p>Hay varias formas de elegir la mejor acción para ejecutar en cada esstado. Primero, cuando estamos en cierto estado <span class="math inline">\(s_t\)</span>, simplemente podríamos tomar la acción <span class="math inline">\(a_t\)</span> que maximiza el valor Q $ Q(s_t, a_t)$:</p>
<p><span class="math display">\[a_t = \underset{a}{\textrm{argmax}}(Q(s_t,a))\]</span></p>
<p>Esta solución es el método <strong>Argmax</strong>.</p>
<p>Otra gran solución, que resulta ser una solución aún mejor para problemas complejos, es el método <strong>Softmax</strong>.</p>
<p>El método Softmax consiste en considerar para cada estado <span class="math inline">\(s\)</span> la siguiente distribución:</p>
<p><span class="math display">\[W_s: a \in A \mapsto \frac{\exp(Q(s,a))^{\tau}}{\sum_{a&#39;}\exp(Q(s,a&#39;))^{\tau}} \textrm{ with } \tau \ge 0\]</span></p>
<p>Luego, elegimos qué acción <span class="math inline">\(a\)</span> llevar a cabo mediante una muestra de un valor aleatorio de esa distribución:</p>
<p><span class="math display">\[a \sim W_s(.)\]</span></p>
<p>Sin embargo, el problema que resolveremos en el Caso Práctico 1 será lo suficientemente simple como para usar el método Argmax, así que esto es lo que elegiremos.</p>
</div>
<div id="el-algoritmo-de-q-learning-al-completo" class="section level3">
<h3><span class="header-section-number">1.2.3</span> El algoritmo de Q-Learning al completo</h3>
<p>Resumamos los diferentes pasos de todo el proceso de Q-Learning:</p>
<p><strong>Inicialización</strong></p>
<p>Para todas las parejas de estados <span class="math inline">\(s\)</span> y acciones <span class="math inline">\(a\)</span>, los valores Q se inicializan a 0:</p>
<p><span class="math display">\[\forall s \in S, a \in A, Q_0(s,a) = 0\]</span></p>
<p>Comenzamos en el estado inicial <span class="math inline">\(s_0\)</span>. Llevamos a cabo una acción aleatoria posible y llegamos al primer estado <span class="math inline">\(s_1\)</span>.</p>
<p><strong>Para cada instante <span class="math inline">\(t \ge 1\)</span></strong>, repetiremos un cierto número de veces (1000 veces en nuestro código) lo siguiente:</p>
<ol style="list-style-type: decimal">
<li>Seleccionamos un estado aleatorio <span class="math inline">\(s_t\)</span> de nuestros 12 estados posibles:</li>
</ol>
<p><span class="math display">\[s_t = \textrm{random}(0,1,2,3,4,5,6,7,8,9,10,11)\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Llevamos a cabo una acción aleatoria <span class="math inline">\(a_t\)</span> que puede conducir al siguiente estado posible, es decir, de modo que <span class="math inline">\(R(s_t,a_t) &gt; 0\)</span>:</li>
</ol>
<p><span class="math display">\[a_t = \textrm{random}(0,1,2,3,4,5,6,7,8,9,10,11) \textrm{ t.q. } R(s_t,a_t) &gt; 0\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li><p>Llegamos al siguiente estado <span class="math inline">\(s_{t + 1}\)</span> y obtenemos la recompensa <span class="math inline">\(R(s_t,a_t)\)</span></p></li>
<li><p>Calculamos la Diferencia Temporal <span class="math inline">\(TD_t(s_t,a_t)\)</span>:</p></li>
</ol>
<p><span class="math display">\[TD_t(s_t,a_t) = R(s_t,a_t) + \gamma \underset{a}{\max}(Q(s_{t+1},a)) - Q(s_t, a_t)\]</span></p>
<ol start="5" style="list-style-type: decimal">
<li>Actualizamos el valor Q aplicando la ecuación de Bellman:</li>
</ol>
<p><span class="math display">\[Q_t(s_t,a_t) = Q_{t-1}(s_t,a_t) + \alpha TD_t(s_t,a_t)\]</span></p>
</div>
</div>
<div id="implementación" class="section level2">
<h2><span class="header-section-number">1.3</span> Implementación</h2>
<p>Ahora proporcionemos y expliquemos la implementación completa de este modelo de Q-Learning, la solución de nuestro problema de optimización de flujos de almacén.</p>
<p>Primero, comenzamos importando las librerías que se usarán en esta implementación. Estos solo incluyen la biblioteca <code>numpy</code>, que ofrece una forma práctica de trabajar con matrices y operaciones matemáticas:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="co"># Importar las librerías</span></a>
<a class="sourceLine" id="cb1-2" data-line-number="2"><span class="im">import</span> numpy <span class="im">as</span> np</a></code></pre></div>
<p>Luego establecemos los parámetros de nuestro modelo. Estos incluyen el factor de descuento <span class="math inline">\(\gamma\)</span> y la tasa de aprendizaje $$, que como vimos en la Sección 1.2, son los únicos parámetros del algoritmo Q-Learning:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="co"># Configuración de los parámetros gamma y alfa para el Q-Learning</span></a>
<a class="sourceLine" id="cb2-2" data-line-number="2">gamma <span class="op">=</span> <span class="fl">0.75</span></a>
<a class="sourceLine" id="cb2-3" data-line-number="3">alpha <span class="op">=</span> <span class="fl">0.9</span></a></code></pre></div>
<p>Las dos secciones de código anteriores eran simplemente las secciones introductorias, antes de comenzar realmente a construir nuestro modelo de IA. Ahora el siguiente paso es comenzar la primera parte de nuestra implementación: Parte 1 - Definición del entorno. Y para eso, por supuesto, comenzamos definiendo los estados, con un diccionario que asigna los nombres de las ubicaciones (en letras de la A a la L) en los estados (en índices del 0 al 11):</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="co"># PARTE 1 - DEFINICIÓN DEL ENTORNO</span></a>
<a class="sourceLine" id="cb3-2" data-line-number="2"></a>
<a class="sourceLine" id="cb3-3" data-line-number="3"><span class="co"># Definición de los estados</span></a>
<a class="sourceLine" id="cb3-4" data-line-number="4">location_to_state <span class="op">=</span> {<span class="st">&#39;A&#39;</span>: <span class="dv">0</span>,</a>
<a class="sourceLine" id="cb3-5" data-line-number="5">                     <span class="st">&#39;B&#39;</span>: <span class="dv">1</span>,</a>
<a class="sourceLine" id="cb3-6" data-line-number="6">                     <span class="st">&#39;C&#39;</span>: <span class="dv">2</span>,</a>
<a class="sourceLine" id="cb3-7" data-line-number="7">                     <span class="st">&#39;D&#39;</span>: <span class="dv">3</span>,</a>
<a class="sourceLine" id="cb3-8" data-line-number="8">                     <span class="st">&#39;E&#39;</span>: <span class="dv">4</span>,</a>
<a class="sourceLine" id="cb3-9" data-line-number="9">                     <span class="st">&#39;F&#39;</span>: <span class="dv">5</span>,</a>
<a class="sourceLine" id="cb3-10" data-line-number="10">                     <span class="st">&#39;G&#39;</span>: <span class="dv">6</span>,</a>
<a class="sourceLine" id="cb3-11" data-line-number="11">                     <span class="st">&#39;H&#39;</span>: <span class="dv">7</span>,</a>
<a class="sourceLine" id="cb3-12" data-line-number="12">                     <span class="st">&#39;I&#39;</span>: <span class="dv">8</span>,</a>
<a class="sourceLine" id="cb3-13" data-line-number="13">                     <span class="st">&#39;J&#39;</span>: <span class="dv">9</span>,</a>
<a class="sourceLine" id="cb3-14" data-line-number="14">                     <span class="st">&#39;K&#39;</span>: <span class="dv">10</span>,</a>
<a class="sourceLine" id="cb3-15" data-line-number="15">                     <span class="st">&#39;L&#39;</span>: <span class="dv">11</span>}</a></code></pre></div>
<p>Luego definimos las acciones, con una simple lista de índices del 0 al 11. Recuerda que cada índice de acción corresponde al siguiente estado (siguiente ubicación) al que conduce dicha acción:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="co"># Definición de las acciones</span></a>
<a class="sourceLine" id="cb4-2" data-line-number="2">actions <span class="op">=</span> [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">7</span>,<span class="dv">8</span>,<span class="dv">9</span>,<span class="dv">10</span>,<span class="dv">11</span>]</a></code></pre></div>
<p>Y eventualmente, definimos las recompensas, creando una matriz de recompensas, donde las filas corresponden a los estados actuales <span class="math inline">\(s_t\)</span>, las columnas corresponden a las acciones <span class="math inline">\(a_t\)</span> que conducen al siguiente estado <span class="math inline">\(s_{t + 1}\)</span>, y las celdas contienen las recompensas <span class="math inline">\(R(s_t, a_t)\)</span>. Si una celda <span class="math inline">\((s_t, a_t)\)</span> tiene un 1, eso significa que podemos llevar a cabo la acción <span class="math inline">\(a_t\)</span> del estado actual <span class="math inline">\(s_t\)</span> para llegar al siguiente estado <span class="math inline">\(s_{t + 1}\)</span>. Si una celda <span class="math inline">\((s_t, a_t)\)</span> tiene un 0, eso significa que no podemos llevar a cabo la acción <span class="math inline">\(a_t\)</span> del estado actual <span class="math inline">\(s_t\)</span> para llegar a cualquier estado siguiente <span class="math inline">\(s_{t + 1}\)</span>. Y por ahora colocaremos manualmente una alta recompensa (1000) dentro de la celda correspondiente a la ubicación G, porque es la ubicación de máxima prioridad donde el almacén autónomo tiene que ir a recoger los productos. Como la ubicación G ha codificado el estado como índice 6, colocamos una recompensa de 1000 en la celda de la fila 6 y la columna 6. Luego, mejoraremos nuestra solución al implementar una forma automática de ir a la ubicación de máxima prioridad, sin tener que actualizar manualmente la matriz de recompensas y dejándola inicializada con 0s y 1s como debería ser. Pero mientras tanto, aquí está debajo de nuestra matriz de recompensas, incluida la actualización manual:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="co"># Definición de las recompensas</span></a>
<a class="sourceLine" id="cb5-2" data-line-number="2">R <span class="op">=</span> np.array([[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb5-3" data-line-number="3">              [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb5-4" data-line-number="4">              [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb5-5" data-line-number="5">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb5-6" data-line-number="6">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb5-7" data-line-number="7">              [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb5-8" data-line-number="8">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1000</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb5-9" data-line-number="9">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>],</a>
<a class="sourceLine" id="cb5-10" data-line-number="10">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb5-11" data-line-number="11">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb5-12" data-line-number="12">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>],</a>
<a class="sourceLine" id="cb5-13" data-line-number="13">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>]])</a></code></pre></div>
<p>Eso finaliza esta primera parte. Ahora comencemos la segunda parte de nuestra implementación: Parte 2 - Construcción la solución de IA con Q-Learning. En ese sentido, vamos a seguir el algoritmo de Q-Learning exactamente como lo vimos en la Sección 1.2. Por lo tanto, primero inicializamos todos los valores Q, creando nuestra matriz de valores Q llena de ceros (en los cuales, las filas corresponden a los estados actuales <span class="math inline">\(s_t\)</span>, las columnas corresponden a las acciones <span class="math inline">\(a_t\)</span> que conducen al siguiente estado $s_{t + 1} $, y las celdas contienen los valores Q, $Q (s_t, a_t)) $:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" data-line-number="1"><span class="co"># PARTE 2 - CONSTRUCCIÓN DE LA SOLUCIÓN DE IA CON Q-LEARNING</span></a>
<a class="sourceLine" id="cb6-2" data-line-number="2"></a>
<a class="sourceLine" id="cb6-3" data-line-number="3"><span class="co"># Inicialización de los valores Q</span></a>
<a class="sourceLine" id="cb6-4" data-line-number="4">Q <span class="op">=</span> np.array(np.zeros([<span class="dv">12</span>,<span class="dv">12</span>]))</a></code></pre></div>
<p>Luego, por supuesto, implementamos el proceso de Q-Learning, con un bucle <code>for</code> que llevará a cabo un total de 1000 iteraciones, repitiendo 1000 veces los pasos del proceso de Q-Learning que analizamos a fondo al final de la Sección 1.2:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" data-line-number="1"><span class="co"># Implementación del proceso de Q-Learning</span></a>
<a class="sourceLine" id="cb7-2" data-line-number="2"><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</a>
<a class="sourceLine" id="cb7-3" data-line-number="3">    current_state <span class="op">=</span> np.random.randint(<span class="dv">0</span>,<span class="dv">12</span>)</a>
<a class="sourceLine" id="cb7-4" data-line-number="4">    playable_actions <span class="op">=</span> []</a>
<a class="sourceLine" id="cb7-5" data-line-number="5">    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">12</span>):</a>
<a class="sourceLine" id="cb7-6" data-line-number="6">        <span class="cf">if</span> R[current_state, j] <span class="op">&gt;</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb7-7" data-line-number="7">            playable_actions.append(j)</a>
<a class="sourceLine" id="cb7-8" data-line-number="8">    next_state <span class="op">=</span> np.random.choice(playable_actions)</a>
<a class="sourceLine" id="cb7-9" data-line-number="9">    TD <span class="op">=</span> R[current_state, next_state] <span class="op">+</span> gamma<span class="op">*</span>Q[next_state, np.argmax(Q[next_state,])]</a>
<a class="sourceLine" id="cb7-10" data-line-number="10">         <span class="op">-</span> Q[current_state, next_state]</a>
<a class="sourceLine" id="cb7-11" data-line-number="11">    Q[current_state, next_state] <span class="op">=</span> Q[current_state, next_state] <span class="op">+</span> alpha<span class="op">*</span>TD</a></code></pre></div>
<p>Opcional: en esta etapa del código, nuestra matriz de valores Q está lista. Podemos echarle un vistazo ejecutando todo el código que hemos implementado hasta ahora e ingresando las siguientes dos instrucciones en la consola:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb8-1" data-line-number="1"><span class="bu">print</span>(<span class="st">&quot;Q-Values:&quot;</span>)</a>
<a class="sourceLine" id="cb8-2" data-line-number="2"><span class="bu">print</span>(Q.astype(<span class="bu">int</span>))</a></code></pre></div>
<p>Y obtenemos la siguiente matriz de valores Q finales:</p>
<p><img src="Images/Q_Values_Console.png" /></p>
<p>Para una mayor claridad visual, incluso puede verificarse la matriz de valores Q directamente en el Explorador de variables, haciendo doble clic en la variable Q. Luego, para obtener los valores Q como enteros, es conveniente hacer clic en <code>Formato</code> e ingresar un formato de como floante como <code>%.0f</code>. Se obtiene en este caso el siguiente resultado, que es un poco más claro ya que se pueden ver en la matriz Q los índices de las filas y columnas de la misma:</p>
<p><img src="Images/Q_Values_Variable_Explorer.png" /></p>
<p>Bien, ahora que tenemos nuestra matriz de valores Q, ¡estamos listos para llevarlo a producción! Por lo tanto, podemos pasar a la tercera parte de la implementación, Parte 3: Poner el modelo en producción, dentro de la cual calcularemos la ruta óptima desde cualquier ubicación inicial a cualquier ubicación final de máxima prioridad. La idea aquí será implementar una función de <em>ruta</em>, que tomará como entradas la ubicación de inicio donde se encuentra nuestro robot de almacén autónomo en un momento específico y la ubicación de finalización donde tiene que ir con la máxima prioridad, y eso volverá como genera la ruta más corta dentro de una lista. Sin embargo, dado que queremos indicar las ubicaciones con sus nombres (en letras), a diferencia de sus estados (en índices), necesitaremos un diccionario que asigne los estados de ubicaciones (en índices) a los nombres de ubicaciones (en letras). Y eso es lo primero que haremos aquí en esta tercera parte, usando un truco para invertir nuestro diccionario anterior <code>location\_to\_state</code>, ya que de hecho simplemente queremos obtener el mapeo inverso exacto de este diccionario:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb9-1" data-line-number="1"><span class="co"># PARTE 3 - PONER EL MODELO EN PRODUCCIÓN</span></a>
<a class="sourceLine" id="cb9-2" data-line-number="2"></a>
<a class="sourceLine" id="cb9-3" data-line-number="3"><span class="co"># Hacer un mapeo de los estados a las ubicaciones</span></a>
<a class="sourceLine" id="cb9-4" data-line-number="4">state_to_location <span class="op">=</span> {state: location <span class="cf">for</span> location, state <span class="kw">in</span> location_to_state.items()}</a></code></pre></div>
<p>Aquí es cuando entra en juego la sección de código más importante. Estamos a punto de implementar la función final <code>route ()</code> que tomará como entradas las ubicaciones de inicio y finalización, y que devolverá la ruta óptima entre estas dos ubicaciones. Para explicar exactamente qué hará esta función de ruta, enumeremos los diferentes pasos del proceso, al pasar de la ubicación E a la ubicación G:</p>
<ol style="list-style-type: decimal">
<li>Comenzamos en nuestra ubicación inicial E.</li>
<li>Obtenemos el estado de ubicación E, que según nuestro mapeo <code>location_to_state</code> es <span class="math inline">\(s_0 = 4\)</span>.</li>
<li>En la fila del estado <span class="math inline">\(s_0 = 4\)</span> de nuestra matriz de valores Q, hallamos la columna con el mayor valor Q (703).</li>
<li>Esta columna tiene el índice 8, por lo que ejecutamos la acción del índice 8 que nos lleva al siguiente estado <span class="math inline">\(s_{t+1} = 8\)</span>.</li>
<li>Obtenemos la ubicación del estado 8, que según nuestro mapeo <code>state_to_location</code> es la ubicación I. Por lo tanto, nuestra próxima ubicación es la ubicación I, que se adjunta a nuestra lista que contiene la ruta óptima global.</li>
<li>Repetimos los mismos 5 pasos anteriores desde nuestra nueva ubicación inicial I, hasta llegar a nuestro destino final, la ubicación G.</li>
</ol>
<p>Por lo tanto, dado que no sabemos cuántas ubicaciones tendremos que atravesar entre las ubicaciones inicial y final, tenemos que hacer un bucle <code>while</code> que repetirá el proceso de 5 pasos descrito anteriormente, y que se detendrá tan pronto como lo hagamos llegar a la ubicación final de máxima prioridad:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb10-1" data-line-number="1"><span class="co"># Hacer la función final que devolverá la ruta óptima</span></a>
<a class="sourceLine" id="cb10-2" data-line-number="2"><span class="kw">def</span> route(starting_location, ending_location):</a>
<a class="sourceLine" id="cb10-3" data-line-number="3">    route <span class="op">=</span> [starting_location]</a>
<a class="sourceLine" id="cb10-4" data-line-number="4">    next_location <span class="op">=</span> starting_location</a>
<a class="sourceLine" id="cb10-5" data-line-number="5">    <span class="cf">while</span> (next_location <span class="op">!=</span> ending_location):</a>
<a class="sourceLine" id="cb10-6" data-line-number="6">        starting_state <span class="op">=</span> location_to_state[starting_location]</a>
<a class="sourceLine" id="cb10-7" data-line-number="7">        next_state <span class="op">=</span> np.argmax(Q[starting_state,])</a>
<a class="sourceLine" id="cb10-8" data-line-number="8">        next_location <span class="op">=</span> state_to_location[next_state]</a>
<a class="sourceLine" id="cb10-9" data-line-number="9">        route.append(next_location)</a>
<a class="sourceLine" id="cb10-10" data-line-number="10">        starting_location <span class="op">=</span> next_location</a>
<a class="sourceLine" id="cb10-11" data-line-number="11">    <span class="cf">return</span> route</a></code></pre></div>
<p>¡Felicidades, nuestra herramienta ya está lista! Cuando lo probamos para ir de E a G, obtenemos las dos rutas óptimas posibles después de imprimir la ruta final ejecutando el código completo varias veces:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb11-1" data-line-number="1"><span class="co"># Imprimir la ruta final</span></a>
<a class="sourceLine" id="cb11-2" data-line-number="2"><span class="bu">print</span>(<span class="st">&#39;Route:&#39;</span>)</a>
<a class="sourceLine" id="cb11-3" data-line-number="3">route(<span class="st">&#39;E&#39;</span>, <span class="st">&#39;G&#39;</span>)</a></code></pre></div>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb12-1" data-line-number="1">Route:</a>
<a class="sourceLine" id="cb12-2" data-line-number="2">Out[<span class="dv">1</span>]: [<span class="st">&#39;E&#39;</span>, <span class="st">&#39;I&#39;</span>, <span class="st">&#39;J&#39;</span>, <span class="st">&#39;F&#39;</span>, <span class="st">&#39;B&#39;</span>, <span class="st">&#39;C&#39;</span>, <span class="st">&#39;G&#39;</span>]</a>
<a class="sourceLine" id="cb12-3" data-line-number="3">Out[<span class="dv">2</span>]: [<span class="st">&#39;E&#39;</span>, <span class="st">&#39;I&#39;</span>, <span class="st">&#39;J&#39;</span>, <span class="st">&#39;K&#39;</span>, <span class="st">&#39;L&#39;</span>, <span class="st">&#39;H&#39;</span>, <span class="st">&#39;G&#39;</span>]</a></code></pre></div>
<p>Bien, tenemos una primera versión del modelo que funciona bien. Pero podemos mejorarlo de dos maneras. Primero, al automatizar la atribución de recompensas a la ubicación de máxima prioridad, para que no tengamos que hacerlo manualmente. Y segundo, al agregar una función que nos da la opción de ir a una ubicación intermedia antes de ir a la ubicación de máxima prioridad. Esa ubicación intermedia debe estar, por supuesto, en las 3 ubicaciones prioritarias principales. Y, de hecho, en nuestra clasificación de ubicaciones de máxima prioridad, la segunda ubicación de máxima prioridad es la ubicación K. Por lo tanto, para optimizar aún más los flujos de almacén, nuestro robot de almacén autónomo debe ir por la ubicación K para recoger los productos en su camino a la ubicación de máxima prioridad G. Una forma de hacer esto es tener la opción de ir a cualquier ubicación intermedia en el proceso de nuestra función <code>route()</code>. Y esto es exactamente lo que implementaremos como segunda mejora. Pero primero, implementemos la primera mejora, que automatiza la atribución de recompensas.</p>
<p>La forma de hacerlo es en dos pasos: primero debemos hacer una copia (llamada <code>R_new</code>) de nuestra matriz de recompensa dentro de la cual la función <code>route()</code> actualizará automáticamente la recompensa en la celda de la ubicación final. De hecho, la ubicación final es una de las entradas de la función <code>route()</code>, por lo que al usar nuestro diccionario de <code>location_to_state</code> podemos encontrar fácilmente esa celda y actualizar su recompensa a 1000. Y segundo, debemos incluir toda la lógica del algoritmo de Q-learning (incluido el paso de inicialización) dentro de la función de ruta, justo después de hacer esa actualización de la recompensa en nuestra copia de la matriz de recompensas. De hecho, en nuestra implementación anterior anterior, el proceso de Q-Learning ocurre en la versión original de la matriz de recompensas, que ahora se supone que permanece como está, es decir, se inicializa solo a 1s y 0s. Por lo tanto, debemos incluir el proceso de Q-Learning dentro de la función de ruta y hacer que suceda en nuestra copia <code>R_new</code> de la matriz de recompensas, en lugar de la matriz de recompensas original <code>R</code>. Por lo tanto, nuestra implementación completa se convierte en lo siguiente:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb13-1" data-line-number="1"><span class="co"># Inteligencia Artificial aplicada a Negocios y Empresas</span></a>
<a class="sourceLine" id="cb13-2" data-line-number="2"></a>
<a class="sourceLine" id="cb13-3" data-line-number="3"><span class="co"># Optimización de Procesos en un almacén con Q-Learning</span></a>
<a class="sourceLine" id="cb13-4" data-line-number="4"></a>
<a class="sourceLine" id="cb13-5" data-line-number="5"><span class="co"># # Importar las librerías</span></a>
<a class="sourceLine" id="cb13-6" data-line-number="6"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb13-7" data-line-number="7"></a>
<a class="sourceLine" id="cb13-8" data-line-number="8"><span class="co"># Configuración de los parámetros gamma y alfa para el Q-Learning</span></a>
<a class="sourceLine" id="cb13-9" data-line-number="9">gamma <span class="op">=</span> <span class="fl">0.75</span></a>
<a class="sourceLine" id="cb13-10" data-line-number="10">alpha <span class="op">=</span> <span class="fl">0.9</span></a>
<a class="sourceLine" id="cb13-11" data-line-number="11"></a>
<a class="sourceLine" id="cb13-12" data-line-number="12"><span class="co"># PARTE 1 - DEFINICIÓN DEL ENTORNO</span></a>
<a class="sourceLine" id="cb13-13" data-line-number="13"></a>
<a class="sourceLine" id="cb13-14" data-line-number="14"><span class="co"># Definición de los estados</span></a>
<a class="sourceLine" id="cb13-15" data-line-number="15">location_to_state <span class="op">=</span> {<span class="st">&#39;A&#39;</span>: <span class="dv">0</span>,</a>
<a class="sourceLine" id="cb13-16" data-line-number="16">                     <span class="st">&#39;B&#39;</span>: <span class="dv">1</span>,</a>
<a class="sourceLine" id="cb13-17" data-line-number="17">                     <span class="st">&#39;C&#39;</span>: <span class="dv">2</span>,</a>
<a class="sourceLine" id="cb13-18" data-line-number="18">                     <span class="st">&#39;D&#39;</span>: <span class="dv">3</span>,</a>
<a class="sourceLine" id="cb13-19" data-line-number="19">                     <span class="st">&#39;E&#39;</span>: <span class="dv">4</span>,</a>
<a class="sourceLine" id="cb13-20" data-line-number="20">                     <span class="st">&#39;F&#39;</span>: <span class="dv">5</span>,</a>
<a class="sourceLine" id="cb13-21" data-line-number="21">                     <span class="st">&#39;G&#39;</span>: <span class="dv">6</span>,</a>
<a class="sourceLine" id="cb13-22" data-line-number="22">                     <span class="st">&#39;H&#39;</span>: <span class="dv">7</span>,</a>
<a class="sourceLine" id="cb13-23" data-line-number="23">                     <span class="st">&#39;I&#39;</span>: <span class="dv">8</span>,</a>
<a class="sourceLine" id="cb13-24" data-line-number="24">                     <span class="st">&#39;J&#39;</span>: <span class="dv">9</span>,</a>
<a class="sourceLine" id="cb13-25" data-line-number="25">                     <span class="st">&#39;K&#39;</span>: <span class="dv">10</span>,</a>
<a class="sourceLine" id="cb13-26" data-line-number="26">                     <span class="st">&#39;L&#39;</span>: <span class="dv">11</span>}</a>
<a class="sourceLine" id="cb13-27" data-line-number="27"></a>
<a class="sourceLine" id="cb13-28" data-line-number="28"><span class="co"># Definición de las acciones</span></a>
<a class="sourceLine" id="cb13-29" data-line-number="29">actions <span class="op">=</span> [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">7</span>,<span class="dv">8</span>,<span class="dv">9</span>,<span class="dv">10</span>,<span class="dv">11</span>]</a>
<a class="sourceLine" id="cb13-30" data-line-number="30"></a>
<a class="sourceLine" id="cb13-31" data-line-number="31"><span class="co"># Definición de las recompensas</span></a>
<a class="sourceLine" id="cb13-32" data-line-number="32">R <span class="op">=</span> np.array([[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb13-33" data-line-number="33">              [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb13-34" data-line-number="34">              [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb13-35" data-line-number="35">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb13-36" data-line-number="36">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb13-37" data-line-number="37">              [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb13-38" data-line-number="38">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb13-39" data-line-number="39">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>],</a>
<a class="sourceLine" id="cb13-40" data-line-number="40">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb13-41" data-line-number="41">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb13-42" data-line-number="42">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>],</a>
<a class="sourceLine" id="cb13-43" data-line-number="43">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>]])</a>
<a class="sourceLine" id="cb13-44" data-line-number="44"></a>
<a class="sourceLine" id="cb13-45" data-line-number="45"><span class="co"># PARTE 2 - CONSTRUCCIÓN DE LA SOLUCIÓN DE IA CON Q-LEARNING</span></a>
<a class="sourceLine" id="cb13-46" data-line-number="46"></a>
<a class="sourceLine" id="cb13-47" data-line-number="47"><span class="co"># Crear una función de mapping desde los estados a las ubicaciones</span></a>
<a class="sourceLine" id="cb13-48" data-line-number="48">state_to_location <span class="op">=</span> {state: location <span class="cf">for</span> location, state <span class="kw">in</span> location_to_state.items()}</a>
<a class="sourceLine" id="cb13-49" data-line-number="49"></a>
<a class="sourceLine" id="cb13-50" data-line-number="50"><span class="co"># Crear una función que devuelva el camino más corto desde la ubicación inicial a la final</span></a>
<a class="sourceLine" id="cb13-51" data-line-number="51"><span class="kw">def</span> route(starting_location, ending_location):</a>
<a class="sourceLine" id="cb13-52" data-line-number="52">    R_new <span class="op">=</span> np.copy(R)</a>
<a class="sourceLine" id="cb13-53" data-line-number="53">    ending_state <span class="op">=</span> location_to_state[ending_location]</a>
<a class="sourceLine" id="cb13-54" data-line-number="54">    R_new[ending_state, ending_state] <span class="op">=</span> <span class="dv">1000</span></a>
<a class="sourceLine" id="cb13-55" data-line-number="55">    Q <span class="op">=</span> np.array(np.zeros([<span class="dv">12</span>,<span class="dv">12</span>]))</a>
<a class="sourceLine" id="cb13-56" data-line-number="56">    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</a>
<a class="sourceLine" id="cb13-57" data-line-number="57">        current_state <span class="op">=</span> np.random.randint(<span class="dv">0</span>,<span class="dv">12</span>)</a>
<a class="sourceLine" id="cb13-58" data-line-number="58">        playable_actions <span class="op">=</span> []</a>
<a class="sourceLine" id="cb13-59" data-line-number="59">        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">12</span>):</a>
<a class="sourceLine" id="cb13-60" data-line-number="60">            <span class="cf">if</span> R_new[current_state, j] <span class="op">&gt;</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb13-61" data-line-number="61">                playable_actions.append(j)</a>
<a class="sourceLine" id="cb13-62" data-line-number="62">        next_state <span class="op">=</span> np.random.choice(playable_actions)</a>
<a class="sourceLine" id="cb13-63" data-line-number="63">        TD <span class="op">=</span> R_new[current_state, next_state]</a>
<a class="sourceLine" id="cb13-64" data-line-number="64">             <span class="op">+</span> gamma <span class="op">*</span> Q[next_state, np.argmax(Q[next_state,])]</a>
<a class="sourceLine" id="cb13-65" data-line-number="65">             <span class="op">-</span> Q[current_state, next_state]</a>
<a class="sourceLine" id="cb13-66" data-line-number="66">        Q[current_state, next_state] <span class="op">=</span> Q[current_state, next_state] <span class="op">+</span> alpha <span class="op">*</span> TD</a>
<a class="sourceLine" id="cb13-67" data-line-number="67">    route <span class="op">=</span> [starting_location]</a>
<a class="sourceLine" id="cb13-68" data-line-number="68">    next_location <span class="op">=</span> starting_location</a>
<a class="sourceLine" id="cb13-69" data-line-number="69">    <span class="cf">while</span> (next_location <span class="op">!=</span> ending_location):</a>
<a class="sourceLine" id="cb13-70" data-line-number="70">        starting_state <span class="op">=</span> location_to_state[starting_location]</a>
<a class="sourceLine" id="cb13-71" data-line-number="71">        next_state <span class="op">=</span> np.argmax(Q[starting_state,])</a>
<a class="sourceLine" id="cb13-72" data-line-number="72">        next_location <span class="op">=</span> state_to_location[next_state]</a>
<a class="sourceLine" id="cb13-73" data-line-number="73">        route.append(next_location)</a>
<a class="sourceLine" id="cb13-74" data-line-number="74">        starting_location <span class="op">=</span> next_location</a>
<a class="sourceLine" id="cb13-75" data-line-number="75">    <span class="cf">return</span> route</a>
<a class="sourceLine" id="cb13-76" data-line-number="76"></a>
<a class="sourceLine" id="cb13-77" data-line-number="77"><span class="co"># PARTE 3 - PONER EL MODELO EN PRODUCCIÓN</span></a>
<a class="sourceLine" id="cb13-78" data-line-number="78"></a>
<a class="sourceLine" id="cb13-79" data-line-number="79"><span class="co"># Imprimir la ruta final</span></a>
<a class="sourceLine" id="cb13-80" data-line-number="80"><span class="bu">print</span>(<span class="st">&#39;Route:&#39;</span>)</a>
<a class="sourceLine" id="cb13-81" data-line-number="81">route(<span class="st">&#39;E&#39;</span>, <span class="st">&#39;G&#39;</span>)</a></code></pre></div>
<p>Al ejecutar este nuevo código varias veces, obtenemos, por supuesto, las mismas dos posibles rutas óptimas que antes.</p>
<p>Ahora abordemos la segunda mejora. Hay tres formas de agregar la opción de ir por la ubicación intermedia K, la segunda ubicación de máxima prioridad:</p>
<ol style="list-style-type: decimal">
<li>Otorgamos una alta recompensa a la acción que lleva de la ubicación J a la ubicación K. Esta alta recompensa debe ser mayor que 1 y menor a 1000. De hecho, debe ser mayor que 1 para que el proceso de Q-Learning favorezca la acción que lleva de J a K, en oposición a la acción que lleva de J a F que tiene la recompensa 1. Y debe ser inferior a 1000, por lo que debemos mantener la recompensa más alta en la ubicación de mayor prioridad para asegurarnos de que terminemos allí. Por lo tanto, por ejemplo, en nuestra matriz de recompensas podemos dar una alta recompensa de 500 a la celda en la fila del índice 9 y la columna del índice 10, ya que de hecho esa celda corresponde a la acción que conduce desde la ubicación J (índice de estado 9) a ubicación K (índice de estado 10). De esa manera, nuestro robot de almacén autónomo siempre irá por la ubicación K en su camino hacia la ubicación G. Así es como sería la matriz de recompensas en ese caso:</li>
</ol>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb14-1" data-line-number="1"><span class="co"># Definición de las recompensas</span></a>
<a class="sourceLine" id="cb14-2" data-line-number="2">R <span class="op">=</span> np.array([[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb14-3" data-line-number="3">          [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb14-4" data-line-number="4">          [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb14-5" data-line-number="5">          [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb14-6" data-line-number="6">          [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb14-7" data-line-number="7">          [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb14-8" data-line-number="8">          [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb14-9" data-line-number="9">          [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>],</a>
<a class="sourceLine" id="cb14-10" data-line-number="10">          [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb14-11" data-line-number="11">          [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">500</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb14-12" data-line-number="12">          [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>],</a>
<a class="sourceLine" id="cb14-13" data-line-number="13">          [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>]])</a></code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li>Damos una mala recompensa a la acción que lleva de la ubicación J a la ubicación F. Esta mala recompensa solo tiene que ser inferior a 0. De hecho, al castigar esta acción con una mala recompensa, el proceso de Q-Learning nunca favorecerá esa acción que lleva de J a F. Por lo tanto, por ejemplo, en nuestra matriz de recompensas podemos dar una mala recompensa de -500 a la celda en la fila del índice 9 y la columna del índice 5, ya que esa celda corresponde a la acción que conduce desde la ubicación J (estado con índice 9) a la ubicación F (estado con índice 5). De esa manera, nuestro robot de almacén autónomo nunca pasará por la ubicación F en su camino hacia la ubicación G. Así es como sería la matriz de recompensas en ese caso:</li>
</ol>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb15-1" data-line-number="1"><span class="co"># Definición de las recompensas</span></a>
<a class="sourceLine" id="cb15-2" data-line-number="2">R <span class="op">=</span> np.array([[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb15-3" data-line-number="3">          [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb15-4" data-line-number="4">          [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb15-5" data-line-number="5">          [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb15-6" data-line-number="6">          [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb15-7" data-line-number="7">          [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb15-8" data-line-number="8">          [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb15-9" data-line-number="9">          [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>],</a>
<a class="sourceLine" id="cb15-10" data-line-number="10">          [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb15-11" data-line-number="11">          [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="op">-</span><span class="dv">500</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb15-12" data-line-number="12">          [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>],</a>
<a class="sourceLine" id="cb15-13" data-line-number="13">          [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>]])</a></code></pre></div>
<ol start="3" style="list-style-type: decimal">
<li>Realizamos una función adicional <code>best_route()</code>, tomando como entradas las tres ubicaciones inicial, intermedia y final, que llamará a nuestra función <code>route()</code> anterior dos veces, una primera vez desde la ubicación inicial a la ubicación intermedia, y un segunda vez desde la ubicación intermedia hasta la ubicación final.</li>
</ol>
<p>Las dos primeras ideas son fáciles de implementar manualmente, pero muy difíciles de implementar automáticamente. De hecho, es fácil encontrar automáticamente el índice de la ubicación intermedia donde queremos ir, pero es muy difícil obtener el índice de la ubicación que lleva a esa ubicación intermedia, ya que depende de la ubicación inicial y la ubicación final. Puedes intentar implementar la primera o la segunda idea, y verás lo que quiero decir. En consecuencia, implementaremos la tercera idea, que puede programarse en solo dos líneas adicionales de código:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb16-1" data-line-number="1"><span class="co"># Hacer la función final que devuelve la ruta óptima</span></a>
<a class="sourceLine" id="cb16-2" data-line-number="2"><span class="kw">def</span> best_route(starting_location, intermediary_location, ending_location):</a>
<a class="sourceLine" id="cb16-3" data-line-number="3">    <span class="cf">return</span> route(starting_location, intermediary_location)</a>
<a class="sourceLine" id="cb16-4" data-line-number="4">           <span class="op">+</span> route(intermediary_location, ending_location)[<span class="dv">1</span>:]</a></code></pre></div>
<p>Finalmente, el código final que incluye esa mejora importante para la optimización de los flujos de nuestro almacén se convierte en:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb17-1" data-line-number="1"><span class="co"># Inteligencia Artificial aplicada a Negocios y Empresas</span></a>
<a class="sourceLine" id="cb17-2" data-line-number="2"><span class="co"># Optimización de Procesos en un almacén con Q-Learning</span></a>
<a class="sourceLine" id="cb17-3" data-line-number="3"></a>
<a class="sourceLine" id="cb17-4" data-line-number="4"><span class="co"># # Importar las librerías</span></a>
<a class="sourceLine" id="cb17-5" data-line-number="5"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb17-6" data-line-number="6"></a>
<a class="sourceLine" id="cb17-7" data-line-number="7"><span class="co"># Configuración de los parámetros gamma y alfa para el Q-Learning</span></a>
<a class="sourceLine" id="cb17-8" data-line-number="8">gamma <span class="op">=</span> <span class="fl">0.75</span></a>
<a class="sourceLine" id="cb17-9" data-line-number="9">alpha <span class="op">=</span> <span class="fl">0.9</span></a>
<a class="sourceLine" id="cb17-10" data-line-number="10"></a>
<a class="sourceLine" id="cb17-11" data-line-number="11"><span class="co"># PARTE 1 - DEFINICIÓN DEL ENTORNO</span></a>
<a class="sourceLine" id="cb17-12" data-line-number="12"></a>
<a class="sourceLine" id="cb17-13" data-line-number="13"><span class="co"># Definición de los estados</span></a>
<a class="sourceLine" id="cb17-14" data-line-number="14">location_to_state <span class="op">=</span> {<span class="st">&#39;A&#39;</span>: <span class="dv">0</span>,</a>
<a class="sourceLine" id="cb17-15" data-line-number="15">                     <span class="st">&#39;B&#39;</span>: <span class="dv">1</span>,</a>
<a class="sourceLine" id="cb17-16" data-line-number="16">                     <span class="st">&#39;C&#39;</span>: <span class="dv">2</span>,</a>
<a class="sourceLine" id="cb17-17" data-line-number="17">                     <span class="st">&#39;D&#39;</span>: <span class="dv">3</span>,</a>
<a class="sourceLine" id="cb17-18" data-line-number="18">                     <span class="st">&#39;E&#39;</span>: <span class="dv">4</span>,</a>
<a class="sourceLine" id="cb17-19" data-line-number="19">                     <span class="st">&#39;F&#39;</span>: <span class="dv">5</span>,</a>
<a class="sourceLine" id="cb17-20" data-line-number="20">                     <span class="st">&#39;G&#39;</span>: <span class="dv">6</span>,</a>
<a class="sourceLine" id="cb17-21" data-line-number="21">                     <span class="st">&#39;H&#39;</span>: <span class="dv">7</span>,</a>
<a class="sourceLine" id="cb17-22" data-line-number="22">                     <span class="st">&#39;I&#39;</span>: <span class="dv">8</span>,</a>
<a class="sourceLine" id="cb17-23" data-line-number="23">                     <span class="st">&#39;J&#39;</span>: <span class="dv">9</span>,</a>
<a class="sourceLine" id="cb17-24" data-line-number="24">                     <span class="st">&#39;K&#39;</span>: <span class="dv">10</span>,</a>
<a class="sourceLine" id="cb17-25" data-line-number="25">                     <span class="st">&#39;L&#39;</span>: <span class="dv">11</span>}</a>
<a class="sourceLine" id="cb17-26" data-line-number="26"></a>
<a class="sourceLine" id="cb17-27" data-line-number="27"><span class="co"># Definición de las acciones</span></a>
<a class="sourceLine" id="cb17-28" data-line-number="28">actions <span class="op">=</span> [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">7</span>,<span class="dv">8</span>,<span class="dv">9</span>,<span class="dv">10</span>,<span class="dv">11</span>]</a>
<a class="sourceLine" id="cb17-29" data-line-number="29"></a>
<a class="sourceLine" id="cb17-30" data-line-number="30"><span class="co"># Definición de las recompensas</span></a>
<a class="sourceLine" id="cb17-31" data-line-number="31">R <span class="op">=</span> np.array([[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb17-32" data-line-number="32">              [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb17-33" data-line-number="33">              [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb17-34" data-line-number="34">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb17-35" data-line-number="35">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb17-36" data-line-number="36">              [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb17-37" data-line-number="37">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb17-38" data-line-number="38">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>],</a>
<a class="sourceLine" id="cb17-39" data-line-number="39">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb17-40" data-line-number="40">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb17-41" data-line-number="41">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>],</a>
<a class="sourceLine" id="cb17-42" data-line-number="42">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>]])</a>
<a class="sourceLine" id="cb17-43" data-line-number="43"></a>
<a class="sourceLine" id="cb17-44" data-line-number="44"><span class="co"># PARTE 2 - CONSTRUCCIÓN DE LA SOLUCIÓN DE IA CON Q-LEARNING</span></a>
<a class="sourceLine" id="cb17-45" data-line-number="45"></a>
<a class="sourceLine" id="cb17-46" data-line-number="46"><span class="co"># Crear una función de mapping desde los estados a las ubicaciones</span></a>
<a class="sourceLine" id="cb17-47" data-line-number="47">state_to_location <span class="op">=</span> {state: location <span class="cf">for</span> location, state <span class="kw">in</span> location_to_state.items()}</a>
<a class="sourceLine" id="cb17-48" data-line-number="48"></a>
<a class="sourceLine" id="cb17-49" data-line-number="49"><span class="co"># Crear una función que devuelva el camino más corto desde la ubicación inicial a la final</span></a>
<a class="sourceLine" id="cb17-50" data-line-number="50"><span class="kw">def</span> route(starting_location, ending_location):</a>
<a class="sourceLine" id="cb17-51" data-line-number="51">    R_new <span class="op">=</span> np.copy(R)</a>
<a class="sourceLine" id="cb17-52" data-line-number="52">    ending_state <span class="op">=</span> location_to_state[ending_location]</a>
<a class="sourceLine" id="cb17-53" data-line-number="53">    R_new[ending_state, ending_state] <span class="op">=</span> <span class="dv">1000</span></a>
<a class="sourceLine" id="cb17-54" data-line-number="54">    Q <span class="op">=</span> np.array(np.zeros([<span class="dv">12</span>,<span class="dv">12</span>]))</a>
<a class="sourceLine" id="cb17-55" data-line-number="55">    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</a>
<a class="sourceLine" id="cb17-56" data-line-number="56">        current_state <span class="op">=</span> np.random.randint(<span class="dv">0</span>,<span class="dv">12</span>)</a>
<a class="sourceLine" id="cb17-57" data-line-number="57">        playable_actions <span class="op">=</span> []</a>
<a class="sourceLine" id="cb17-58" data-line-number="58">        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">12</span>):</a>
<a class="sourceLine" id="cb17-59" data-line-number="59">            <span class="cf">if</span> R_new[current_state, j] <span class="op">&gt;</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb17-60" data-line-number="60">                playable_actions.append(j)</a>
<a class="sourceLine" id="cb17-61" data-line-number="61">        next_state <span class="op">=</span> np.random.choice(playable_actions)</a>
<a class="sourceLine" id="cb17-62" data-line-number="62">        TD <span class="op">=</span> R_new[current_state, next_state]</a>
<a class="sourceLine" id="cb17-63" data-line-number="63">             <span class="op">+</span> gamma <span class="op">*</span> Q[next_state, np.argmax(Q[next_state,])]</a>
<a class="sourceLine" id="cb17-64" data-line-number="64">             <span class="op">-</span> Q[current_state, next_state]</a>
<a class="sourceLine" id="cb17-65" data-line-number="65">        Q[current_state, next_state] <span class="op">=</span> Q[current_state, next_state] <span class="op">+</span> alpha <span class="op">*</span> TD</a>
<a class="sourceLine" id="cb17-66" data-line-number="66">    route <span class="op">=</span> [starting_location]</a>
<a class="sourceLine" id="cb17-67" data-line-number="67">    next_location <span class="op">=</span> starting_location</a>
<a class="sourceLine" id="cb17-68" data-line-number="68">    <span class="cf">while</span> (next_location <span class="op">!=</span> ending_location):</a>
<a class="sourceLine" id="cb17-69" data-line-number="69">        starting_state <span class="op">=</span> location_to_state[starting_location]</a>
<a class="sourceLine" id="cb17-70" data-line-number="70">        next_state <span class="op">=</span> np.argmax(Q[starting_state,])</a>
<a class="sourceLine" id="cb17-71" data-line-number="71">        next_location <span class="op">=</span> state_to_location[next_state]</a>
<a class="sourceLine" id="cb17-72" data-line-number="72">        route.append(next_location)</a>
<a class="sourceLine" id="cb17-73" data-line-number="73">        starting_location <span class="op">=</span> next_location</a>
<a class="sourceLine" id="cb17-74" data-line-number="74">    <span class="cf">return</span> route</a>
<a class="sourceLine" id="cb17-75" data-line-number="75"></a>
<a class="sourceLine" id="cb17-76" data-line-number="76"><span class="co"># PARTE 3 - PONER EL MODELO EN PRODUCCIÓN</span></a>
<a class="sourceLine" id="cb17-77" data-line-number="77"></a>
<a class="sourceLine" id="cb17-78" data-line-number="78"><span class="co"># Crear la función final que devuelve la ruta óptima</span></a>
<a class="sourceLine" id="cb17-79" data-line-number="79"><span class="kw">def</span> best_route(starting_location, intermediary_location, ending_location):</a>
<a class="sourceLine" id="cb17-80" data-line-number="80">    <span class="cf">return</span> route(starting_location, intermediary_location)</a>
<a class="sourceLine" id="cb17-81" data-line-number="81">           <span class="op">+</span> route(intermediary_location, ending_location)[<span class="dv">1</span>:]</a>
<a class="sourceLine" id="cb17-82" data-line-number="82"></a>
<a class="sourceLine" id="cb17-83" data-line-number="83"><span class="co"># Imprimir la ruta final</span></a>
<a class="sourceLine" id="cb17-84" data-line-number="84"><span class="bu">print</span>(<span class="st">&#39;Route:&#39;</span>)</a>
<a class="sourceLine" id="cb17-85" data-line-number="85">best_route(<span class="st">&#39;E&#39;</span>, <span class="st">&#39;K&#39;</span>, <span class="st">&#39;G&#39;</span>)</a></code></pre></div>
<p>Al ejecutar este código completamente nuevo tantas veces como queramos, siempre obtendremos el mismo resultado esperado:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb18-1" data-line-number="1">Best Route:</a>
<a class="sourceLine" id="cb18-2" data-line-number="2">Out[<span class="dv">1</span>]: [<span class="st">&#39;E&#39;</span>, <span class="st">&#39;I&#39;</span>, <span class="st">&#39;J&#39;</span>, <span class="st">&#39;K&#39;</span>, <span class="st">&#39;L&#39;</span>, <span class="st">&#39;H&#39;</span>, <span class="st">&#39;G&#39;</span>]</a></code></pre></div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="minimización-de-costes.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/joanby/ia4business/edit/master/1.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["curso-ia-business-udemy.pdf", "curso-ia-business-udemy.epub"],
"toc": {
"collapse": "subsection"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
